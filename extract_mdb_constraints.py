"""
æå–MDBæ•°æ®åº“çš„å®Œæ•´å­—æ®µçº¦æŸä¿¡æ¯
ä¸éœ€è¦å®‰è£…Microsoft Accessï¼Œä½¿ç”¨pypyodbcåº“
"""

import sys
import io
# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼Œé¿å…Windowsæ§åˆ¶å°ä¸­æ–‡ä¹±ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import pypyodbc
import pandas as pd
from pathlib import Path
import json
from datetime import datetime

# é…ç½®
MDB_FILES = {
    'course': r'E:\STUDY\0-BIT\5-Y3-1\6-DataWarehouse&Mining\project\APIB24\AP_IB_Course_2024.mdb',
    'assessment': r'E:\STUDY\0-BIT\5-Y3-1\6-DataWarehouse&Mining\project\APIB24\AP_IB_Assessment_2024.mdb'
}

# å®šä¹‰éœ€è¦å®Œæ•´æšä¸¾æ‰€æœ‰å€¼çš„å­—æ®µï¼ˆåŸºäºPDFæ–‡æ¡£åˆ†æï¼‰
FIELDS_TO_ENUMERATE_COMPLETELY = [
    'AGGREGATION_INDEX',
    'AGGREGATION_TYPE', 
    'NRC_CODE',
    'NRC_DESC',
    'SUBGROUP_CODE',      # æœ€é‡è¦ï¼äººå£ç»Ÿè®¡åˆ†ç»„
    'SUBGROUP_NAME',      # æœ€é‡è¦ï¼åˆ†ç»„åç§°
    'APIB_IND',
    'NYC_IND',
    'GRADE_LEVEL',
    'SUBJECT_AREA',
    'COUNTY_CODE',
    'COUNTY_NAME'
]

# å®šä¹‰å»ºè®®æšä¸¾ä½†å¯èƒ½å€¼è¾ƒå¤šçš„å­—æ®µ
FIELDS_TO_ENUMERATE_WITH_LIMIT = [
    'COURSE_ID',
    'COURSE_DESC',
    'STATE_CODE',
    'ITEM_DESC'
]

# å®šä¹‰åªéœ€è¦ç»Ÿè®¡ä¿¡æ¯çš„å­—æ®µï¼ˆä¸æšä¸¾å€¼ï¼‰
FIELDS_STATS_ONLY = [
    'AGGREGATION_CODE',
    'AGGREGATION_NAME',
    'LEA_BEDS',
    'LEA_NAME',
    'INST_ID',
    'STUDENT_COUNT',
    'TESTED_STUDENT_CNT',
    'PROFICIENT_STUDENT_CNT',
    'LEVEL1_CNT',
    'LEVEL2_CNT',
    'LEVEL3_CNT',
    'LEVEL4_CNT',
    'LEVEL5_CNT',
    'LEVEL6_CNT',
    'LEVEL7_CNT'
]


def connect_to_mdb(mdb_path):
    """è¿æ¥åˆ°MDBæ•°æ®åº“"""
    conn_str = f'Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={mdb_path};'
    try:
        conn = pypyodbc.connect(conn_str)
        return conn
    except Exception as e:
        print(f"[é”™è¯¯] è¿æ¥å¤±è´¥: {e}")
        print("\n[è§£å†³æ–¹æ¡ˆ]")
        print("1. å®‰è£… pypyodbc: pip install pypyodbc")
        print("2. å¦‚æœä»ç„¶å¤±è´¥ï¼Œéœ€è¦å®‰è£… Microsoft Access Database Engine 2016 Redistributable")
        print("   ä¸‹è½½åœ°å€: https://www.microsoft.com/en-us/download/details.aspx?id=54920")
        return None


def get_table_schema(conn, table_name):
    """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
    cursor = conn.cursor()
    
    # è·å–å­—æ®µä¿¡æ¯
    # pypyodbcçš„columns()è¿”å›å…ƒç»„ï¼Œå„ä¸ªä½ç½®çš„å«ä¹‰ï¼š
    # 0-table_cat, 1-table_schem, 2-table_name, 3-column_name,
    # 4-data_type, 5-type_name, 6-column_size, 7-buffer_length,
    # 8-decimal_digits, 9-num_prec_radix, 10-nullable, 11-remarks,
    # 12-column_def, 13-sql_data_type, 14-sql_datetime_sub,
    # 15-char_octet_length, 16-ordinal_position, 17-is_nullable
    columns_info = []
    for row in cursor.columns(table=table_name):
        column_info = {
            'column_name': row[3],           # column_name
            'data_type': row[5],             # type_name
            'column_size': row[6],           # column_size
            'nullable': row[10],             # nullable (0=ä¸å…è®¸, 1=å…è®¸)
            'ordinal_position': row[16],     # ordinal_position
            'default': row[12],              # column_defï¼ˆé»˜è®¤å€¼ï¼Œå¯èƒ½ä¸ºNoneï¼‰
            'remarks': row[11],              # remarksï¼ˆå¤‡æ³¨/è¯´æ˜ï¼Œå¯èƒ½ä¸ºNoneï¼‰
        }
        columns_info.append(column_info)
    
    return columns_info


def get_table_constraints(conn, table_name):
    """è·å–ä¸»é”®ã€å¤–é”®ã€ç´¢å¼•/å”¯ä¸€æ€§çº¦æŸä¿¡æ¯"""
    cursor = conn.cursor()

    # ä¸»é”®
    primary_keys = []
    try:
        for row in cursor.primaryKeys(table=table_name):
            # row: table_cat, table_schem, table_name, column_name, key_seq, pk_name
            primary_keys.append({
                'column_name': row[3],
                'key_seq': row[4],
                'pk_name': row[5]
            })
    except Exception as _:
        pass

    # å¤–é”®
    foreign_keys = []
    try:
        for row in cursor.foreignKeys(table=table_name):
            # row: pktable_cat, pktable_schem, pktable_name, pkcolumn_name,
            #      fktable_cat, fktable_schem, fktable_name, fkcolumn_name,
            #      key_seq, update_rule, delete_rule, fk_name, pk_name, deferrability
            foreign_keys.append({
                'pk_table': row[2],
                'pk_column': row[3],
                'fk_table': row[6],
                'fk_column': row[7],
                'key_seq': row[8],
                'fk_name': row[11],
                'pk_name': row[12]
            })
    except Exception as _:
        pass

    # ç´¢å¼•ï¼ˆå«å”¯ä¸€æ€§ï¼‰
    indexes = []
    try:
        for row in cursor.statistics(table=table_name, unique=False):
            # row: table_cat, table_schem, table_name, non_unique, index_qualifier,
            #      index_name, type, ordinal_position, column_name, asc_or_desc,
            #      cardinality, pages, filter_condition
            index_name = row[5]
            if index_name is None:
                continue
            indexes.append({
                'index_name': index_name,
                'non_unique': bool(row[3]),
                'type': row[6],
                'ordinal_position': row[7],
                'column_name': row[8],
                'asc_or_desc': row[9]
            })
    except Exception as _:
        pass

    return {
        'primary_keys': primary_keys,
        'foreign_keys': foreign_keys,
        'indexes': indexes
    }


def perform_rule_checks(conn, table_name, is_assessment=False):
    """ä¸šåŠ¡è§„åˆ™æ ¡éªŒï¼š
    - å¯¹è¯„ä¼°è¡¨ï¼š
      1) tested_student_cnt == sum(level1..levelN)
      2) AP è¾¾æ ‡: proficient == level3+4+5
      3) IB è¾¾æ ‡: proficient == level4+5+6+7
    è¿”å›ä¸ä¸€è‡´è®°å½•è®¡æ•°ä¸æ¯”ä¾‹ã€‚
    """
    cursor = conn.cursor()

    results = {}
    if not is_assessment:
        return results

    # å°†æ–‡æœ¬'-'ä¸NULLè½¬0ï¼Œä½¿ç”¨VALå‡½æ•°å°†æ–‡æœ¬è½¬æ•°å­—
    lvl = lambda c: f"IIF([{c}]='-', 0, VAL([{c}]))"

    try:
        # 1) levelsæ±‚å’Œä¸€è‡´æ€§
        sum_levels = f"{lvl('level1_cnt')}+{lvl('level2_cnt')}+{lvl('level3_cnt')}+{lvl('level4_cnt')}+{lvl('level5_cnt')}+{lvl('level6_cnt')}+{lvl('level7_cnt')}"
        q_total = f"SELECT COUNT(*) FROM [{table_name}]"
        cursor.execute(q_total)
        total_rows = cursor.fetchone()[0]

        q_mismatch_levels = f"""
            SELECT COUNT(*)
            FROM [{table_name}]
            WHERE VAL([{ 'tested_student_cnt' }]) <> ({sum_levels})
        """
        cursor.execute(q_mismatch_levels)
        levels_mismatch = cursor.fetchone()[0]

        results['levels_sum_check'] = {
            'total_rows': total_rows,
            'mismatch_rows': levels_mismatch,
            'mismatch_percentage': round(levels_mismatch / total_rows * 100, 4) if total_rows else 0.0
        }
    except Exception as e:
        results['levels_sum_check_error'] = str(e)

    try:
        # 2) AP è¾¾æ ‡ï¼ˆAPIB_IND='AP'ï¼‰ï¼šproficient == level3+4+5
        ap_sum = f"{lvl('level3_cnt')}+{lvl('level4_cnt')}+{lvl('level5_cnt')}"
        q_ap_total = f"SELECT COUNT(*) FROM [{table_name}] WHERE [APIB_IND]='AP'"
        cursor.execute(q_ap_total)
        ap_total = cursor.fetchone()[0]
        q_ap_mismatch = f"""
            SELECT COUNT(*)
            FROM [{table_name}]
            WHERE [APIB_IND]='AP'
              AND VAL([{ 'proficient_student_cnt' }]) <> ({ap_sum})
        """
        cursor.execute(q_ap_mismatch)
        ap_mismatch = cursor.fetchone()[0]
        results['ap_proficient_check'] = {
            'total_rows': ap_total,
            'mismatch_rows': ap_mismatch,
            'mismatch_percentage': round(ap_mismatch / ap_total * 100, 4) if ap_total else 0.0
        }
    except Exception as e:
        results['ap_proficient_check_error'] = str(e)

    try:
        # 3) IB è¾¾æ ‡ï¼ˆAPIB_IND='IB'ï¼‰ï¼šproficient == level4+5+6+7
        ib_sum = f"{lvl('level4_cnt')}+{lvl('level5_cnt')}+{lvl('level6_cnt')}+{lvl('level7_cnt')}"
        q_ib_total = f"SELECT COUNT(*) FROM [{table_name}] WHERE [APIB_IND]='IB'"
        cursor.execute(q_ib_total)
        ib_total = cursor.fetchone()[0]
        q_ib_mismatch = f"""
            SELECT COUNT(*)
            FROM [{table_name}]
            WHERE [APIB_IND]='IB'
              AND VAL([{ 'proficient_student_cnt' }]) <> ({ib_sum})
        """
        cursor.execute(q_ib_mismatch)
        ib_mismatch = cursor.fetchone()[0]
        results['ib_proficient_check'] = {
            'total_rows': ib_total,
            'mismatch_rows': ib_mismatch,
            'mismatch_percentage': round(ib_mismatch / ib_total * 100, 4) if ib_total else 0.0
        }
    except Exception as e:
        results['ib_proficient_check_error'] = str(e)

    return results


def analyze_field_values(conn, table_name, field_name):
    """åˆ†æå­—æ®µçš„å€¼åˆ†å¸ƒ"""
    cursor = conn.cursor()
    
    try:
        # åŸºæœ¬ç»Ÿè®¡
        query_total = f"SELECT COUNT(*) as total FROM [{table_name}]"
        cursor.execute(query_total)
        total_rows = cursor.fetchone()[0]
        
        # éç©ºå€¼ç»Ÿè®¡
        query_non_null = f"SELECT COUNT([{field_name}]) as non_null FROM [{table_name}]"
        cursor.execute(query_non_null)
        non_null_count = cursor.fetchone()[0]
        
        # è·å–æ‰€æœ‰å”¯ä¸€å€¼åŠå…¶å‡ºç°æ¬¡æ•°ï¼ˆåŒæ—¶å¾—åˆ°distinct_countï¼‰
        # Accessä¸æ”¯æŒCOUNT(DISTINCT)ï¼Œä½†GROUP BYå¯ä»¥åŒæ—¶è·å–å”¯ä¸€å€¼å’Œæ•°é‡
        query_values = f"""
            SELECT [{field_name}], COUNT(*) as count 
            FROM [{table_name}] 
            GROUP BY [{field_name}]
            ORDER BY COUNT(*) DESC
        """
        cursor.execute(query_values)
        values_distribution = cursor.fetchall()
        
        # ä»GROUP BYç»“æœä¸­è·å–å”¯ä¸€å€¼æ•°é‡
        distinct_count = len(values_distribution)
        
        return {
            'total_rows': total_rows,
            'non_null_count': non_null_count,
            'null_count': total_rows - non_null_count,
            'distinct_count': distinct_count,
            'values_distribution': [(str(row[0]) if row[0] is not None else 'NULL', row[1]) for row in values_distribution]
        }
    except Exception as e:
        return {'error': str(e)}


def extract_all_constraints(mdb_path, db_name):
    """æå–æ•°æ®åº“çš„å®Œæ•´çº¦æŸä¿¡æ¯"""
    print(f"\n{'='*80}")
    print(f"[åˆ†æ] æ­£åœ¨åˆ†æ: {db_name}")
    print(f"{'='*80}")
    
    conn = connect_to_mdb(mdb_path)
    if not conn:
        return None
    
    cursor = conn.cursor()
    
    # è·å–æ‰€æœ‰è¡¨å
    tables = []
    for table_info in cursor.tables(tableType='TABLE'):
        # pypyodbcè¿”å›çš„æ˜¯å…ƒç»„ï¼Œè¡¨ååœ¨ç¬¬3ä¸ªä½ç½®ï¼ˆç´¢å¼•2ï¼‰
        table_name = table_info[2]
        if not table_name.startswith('MSys'):  # æ’é™¤ç³»ç»Ÿè¡¨
            tables.append(table_name)
    print(f"\n[å®Œæˆ] æ‰¾åˆ° {len(tables)} ä¸ªè¡¨: {tables}")
    
    database_info = {
        'database_name': db_name,
        'mdb_path': mdb_path,
        'tables': {}
    }
    
    for table_name in tables:
        print(f"\nğŸ“‹ åˆ†æè¡¨: {table_name}")
        
        # è·å–è¡¨ç»“æ„
        schema = get_table_schema(conn, table_name)
        print(f"  â”œâ”€ å­—æ®µæ•°é‡: {len(schema)}")
        
        # è·å–è®°å½•æ•°
        cursor.execute(f"SELECT COUNT(*) FROM [{table_name}]")
        row_count = cursor.fetchone()[0]
        print(f"  â”œâ”€ è®°å½•æ•°é‡: {row_count:,}")
        
        # é‡‡é›†è¡¨çº§çº¦æŸ
        constraints = get_table_constraints(conn, table_name)

        table_info = {
            'row_count': row_count,
            'column_count': len(schema),
            'schema': schema,
            'field_analysis': {},
            'constraints': constraints
        }
        
        # åˆ†ææ¯ä¸ªå­—æ®µ
        for col_info in schema:
            field_name = col_info['column_name']
            print(f"  â”œâ”€ åˆ†æå­—æ®µ: {field_name}", end='')
            
            field_stats = analyze_field_values(conn, table_name, field_name)
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å®Œæ•´æšä¸¾
            enumerate_all = field_name in FIELDS_TO_ENUMERATE_COMPLETELY
            enumerate_limited = field_name in FIELDS_TO_ENUMERATE_WITH_LIMIT
            
            if 'error' not in field_stats:
                distinct_count = field_stats['distinct_count']
                print(f" - {distinct_count} ä¸ªå”¯ä¸€å€¼", end='')
                
                field_analysis = {
                    'data_type': col_info['data_type'],
                    'column_size': col_info['column_size'],
                    'nullable': col_info['nullable'],
                    'total_rows': field_stats['total_rows'],
                    'non_null_count': field_stats['non_null_count'],
                    'null_count': field_stats['null_count'],
                    'null_percentage': round(field_stats['null_count'] / field_stats['total_rows'] * 100, 2),
                    'distinct_count': distinct_count,
                    'distinct_percentage': round(distinct_count / field_stats['non_null_count'] * 100, 2) if field_stats['non_null_count'] > 0 else 0,
                }
                
                # æ ¹æ®å­—æ®µç±»å‹å†³å®šæ˜¯å¦æšä¸¾å€¼
                if enumerate_all:
                    # å®Œæ•´æšä¸¾æ‰€æœ‰å€¼
                    field_analysis['all_values'] = field_stats['values_distribution']
                    print(f" [å®Œæ•´æšä¸¾]")
                elif enumerate_limited and distinct_count <= 500:
                    # æœ‰é™æšä¸¾ï¼ˆæœ€å¤š500ä¸ªï¼‰
                    field_analysis['all_values'] = field_stats['values_distribution']
                    print(f" [æœ‰é™æšä¸¾]")
                elif distinct_count <= 20:
                    # å€¼è¾ƒå°‘ï¼Œè‡ªåŠ¨æšä¸¾
                    field_analysis['all_values'] = field_stats['values_distribution']
                    print(f" [è‡ªåŠ¨æšä¸¾-å€¼å°‘]")
                else:
                    # åªä¿ç•™å‰50ä¸ªæœ€å¸¸è§çš„å€¼
                    field_analysis['top_50_values'] = field_stats['values_distribution'][:50]
                    print(f" [ä»…ç»Ÿè®¡-TOP50]")
                
                table_info['field_analysis'][field_name] = field_analysis
            else:
                print(f" [é”™è¯¯]: {field_stats['error']}")
        
        # ä¸šåŠ¡è§„åˆ™æ ¡éªŒï¼ˆä»…è¯„ä¼°è¡¨ï¼‰
        rule_checks = perform_rule_checks(conn, table_name, is_assessment=(db_name == 'assessment'))
        if rule_checks:
            table_info['rule_checks'] = rule_checks

        database_info['tables'][table_name] = table_info
    
    conn.close()
    return database_info


def save_results(database_info, output_dir='analysis_results'):
    """ä¿å­˜åˆ†æç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    db_name = database_info['database_name']
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. ä¿å­˜å®Œæ•´çš„JSONæ ¼å¼
    json_file = output_path / f'{db_name}_å®Œæ•´çº¦æŸä¿¡æ¯_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(database_info, f, ensure_ascii=False, indent=2)
    print(f"\n[å®Œæˆ] JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
    
    # 2. ä¿å­˜äººç±»å¯è¯»çš„Markdownæ ¼å¼
    md_file = output_path / f'{db_name}_å­—æ®µçº¦æŸæŠ¥å‘Š_{timestamp}.md'
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# {db_name} æ•°æ®åº“å­—æ®µçº¦æŸåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**æ•°æ®åº“è·¯å¾„**: {database_info['mdb_path']}\n\n")
        
        for table_name, table_info in database_info['tables'].items():
            f.write(f"\n## è¡¨: {table_name}\n\n")
            f.write(f"- **è®°å½•æ•°**: {table_info['row_count']:,}\n")
            f.write(f"- **å­—æ®µæ•°**: {table_info['column_count']}\n\n")

            # è¡¨çº§çº¦æŸ
            if 'constraints' in table_info:
                cons = table_info['constraints']
                f.write("### è¡¨çº§çº¦æŸ\n\n")
                # ä¸»é”®
                if cons.get('primary_keys'):
                    pk_cols = ", ".join([c['column_name'] for c in cons['primary_keys']])
                    f.write(f"- ä¸»é”®: {pk_cols}\n")
                else:
                    f.write("- ä¸»é”®: (æ— /æœªæ£€æµ‹åˆ°)\n")
                # å¤–é”®
                if cons.get('foreign_keys'):
                    f.write("- å¤–é”®:\n")
                    for fk in cons['foreign_keys'][:20]:
                        f.write(f"  - {fk['fk_column']} -> {fk['pk_table']}.{fk['pk_column']} (FK: {fk.get('fk_name')})\n")
                    if len(cons['foreign_keys']) > 20:
                        f.write(f"  - ... å…± {len(cons['foreign_keys'])} æ¡\n")
                else:
                    f.write("- å¤–é”®: (æ— /æœªæ£€æµ‹åˆ°)\n")
                # ç´¢å¼•
                if cons.get('indexes'):
                    uniq = [i for i in cons['indexes'] if not i['non_unique']]
                    nonuniq = [i for i in cons['indexes'] if i['non_unique']]
                    f.write(f"- å”¯ä¸€ç´¢å¼•: {len(uniq)} ä¸ªï¼Œæ™®é€šç´¢å¼•: {len(nonuniq)} ä¸ª\n\n")

            f.write("### å­—æ®µè¯¦ç»†ä¿¡æ¯\n\n")
            
            for field_name, field_analysis in table_info['field_analysis'].items():
                f.write(f"\n#### å­—æ®µ: `{field_name}`\n\n")
                f.write(f"**åŸºæœ¬ä¿¡æ¯**:\n")
                f.write(f"- æ•°æ®ç±»å‹: `{field_analysis['data_type']}`\n")
                f.write(f"- å­—æ®µå¤§å°: {field_analysis['column_size']}\n")
                f.write(f"- å…è®¸ç©ºå€¼: {'æ˜¯' if field_analysis['nullable'] else 'å¦'}\n\n")
                # é»˜è®¤å€¼ä¸å¤‡æ³¨
                for col in table_info['schema']:
                    if col['column_name'] == field_name:
                        if col.get('default') is not None:
                            f.write(f"- é»˜è®¤å€¼: {col['default']}\n")
                        if col.get('remarks'):
                            f.write(f"- å¤‡æ³¨: {col['remarks']}\n")
                        break
                
                f.write(f"**ç»Ÿè®¡ä¿¡æ¯**:\n")
                f.write(f"- æ€»è®°å½•æ•°: {field_analysis['total_rows']:,}\n")
                f.write(f"- éç©ºè®°å½•æ•°: {field_analysis['non_null_count']:,}\n")
                f.write(f"- ç©ºå€¼æ•°é‡: {field_analysis['null_count']:,} ({field_analysis['null_percentage']}%)\n")
                # å”¯ä¸€å€¼æ•°é‡çš„ç™¾åˆ†æ¯”è¯´æ˜ï¼šè¡¨ç¤ºå€¼çš„å¤šæ ·æ€§ï¼ˆå¤šæ ·æ€§ = å”¯ä¸€å€¼æ•°/éç©ºè®°å½•æ•°ï¼‰
                # 0.01%è¡¨ç¤ºå€¼éå¸¸é›†ä¸­ï¼ˆå‡ ä¹éƒ½æ˜¯é‡å¤å€¼ï¼‰ï¼Œ100%è¡¨ç¤ºæ¯ä¸ªå€¼éƒ½ä¸åŒ
                distinct_pct_explanation = f" ({field_analysis['distinct_percentage']}% - å€¼å¤šæ ·æ€§æŒ‡æ ‡ï¼Œéè¦†ç›–ç‡)"
                f.write(f"- å”¯ä¸€å€¼æ•°é‡: {field_analysis['distinct_count']:,}{distinct_pct_explanation}\n\n")
                
                # æšä¸¾å€¼
                if 'all_values' in field_analysis:
                    f.write(f"**æ‰€æœ‰å¯èƒ½çš„å€¼** (å…± {len(field_analysis['all_values'])} ä¸ª):\n\n")
                    f.write("| å€¼ | å‡ºç°æ¬¡æ•° | å æ€»è®°å½•ç™¾åˆ†æ¯” |\n")
                    f.write("|---|---|---|\n")
                    for value, count in field_analysis['all_values']:
                        # ç»Ÿä¸€ä½¿ç”¨æ€»è®°å½•æ•°ä½œä¸ºåˆ†æ¯ï¼Œé¿å…NULLå€¼å¯¼è‡´ç™¾åˆ†æ¯”è¶…è¿‡100%
                        percentage = count / field_analysis['total_rows'] * 100 if field_analysis['total_rows'] > 0 else 0
                        f.write(f"| {value} | {count:,} | {percentage:.2f}% |\n")
                elif 'top_50_values' in field_analysis:
                    f.write(f"**å‰50ä¸ªæœ€å¸¸è§çš„å€¼**:\n\n")
                    f.write("| å€¼ | å‡ºç°æ¬¡æ•° | å æ€»è®°å½•ç™¾åˆ†æ¯” |\n")
                    f.write("|---|---|---|\n")
                    for value, count in field_analysis['top_50_values']:
                        # ç»Ÿä¸€ä½¿ç”¨æ€»è®°å½•æ•°ä½œä¸ºåˆ†æ¯
                        percentage = count / field_analysis['total_rows'] * 100 if field_analysis['total_rows'] > 0 else 0
                        f.write(f"| {value} | {count:,} | {percentage:.2f}% |\n")
                
                f.write("\n---\n")

            # è§„åˆ™æ ¡éªŒ
            if 'rule_checks' in table_info and table_info['rule_checks']:
                f.write("\n### è§„åˆ™æ ¡éªŒ\n\n")
                rc = table_info['rule_checks']
                if rc.get('levels_sum_check'):
                    s = rc['levels_sum_check']
                    f.write(f"- Levelsæ±‚å’Œä¸€è‡´æ€§: å¼‚å¸¸ {s['mismatch_rows']:,} / {s['total_rows']:,} ({s['mismatch_percentage']}%)\n")
                if rc.get('ap_proficient_check'):
                    s = rc['ap_proficient_check']
                    f.write(f"- APè¾¾æ ‡ä¸€è‡´æ€§: å¼‚å¸¸ {s['mismatch_rows']:,} / {s['total_rows']:,} ({s['mismatch_percentage']}%)\n")
                if rc.get('ib_proficient_check'):
                    s = rc['ib_proficient_check']
                    f.write(f"- IBè¾¾æ ‡ä¸€è‡´æ€§: å¼‚å¸¸ {s['mismatch_rows']:,} / {s['total_rows']:,} ({s['mismatch_percentage']}%)\n")
    
    print(f"[å®Œæˆ] MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")
    
    # 3. å•ç‹¬ä¿å­˜å…³é”®å­—æ®µçš„å®Œæ•´æšä¸¾å€¼ï¼ˆCSVæ ¼å¼ï¼‰
    for table_name, table_info in database_info['tables'].items():
        for field_name in FIELDS_TO_ENUMERATE_COMPLETELY:
            if field_name in table_info['field_analysis']:
                field_data = table_info['field_analysis'][field_name]
                if 'all_values' in field_data:
                    csv_file = output_path / f'{db_name}_{table_name}_{field_name}_å®Œæ•´æšä¸¾å€¼.csv'
                    df = pd.DataFrame(field_data['all_values'], columns=['å€¼', 'å‡ºç°æ¬¡æ•°'])
                    # ä½¿ç”¨æ€»è®°å½•æ•°ä½œä¸ºåˆ†æ¯è®¡ç®—ç™¾åˆ†æ¯”ï¼Œé¿å…NULLå€¼å¯¼è‡´é—®é¢˜
                    total_rows = field_data['total_rows']
                    df['å æ€»è®°å½•ç™¾åˆ†æ¯”'] = (df['å‡ºç°æ¬¡æ•°'] / total_rows * 100).round(2)
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    print(f"[å®Œæˆ] å­—æ®µæšä¸¾CSVå·²ä¿å­˜: {csv_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("MDBæ•°æ®åº“å­—æ®µçº¦æŸæå–å·¥å…·")
    print("="*80)
    print("\n[é…ç½®ä¿¡æ¯]")
    print(f"   - éœ€è¦å®Œæ•´æšä¸¾çš„å­—æ®µ: {len(FIELDS_TO_ENUMERATE_COMPLETELY)} ä¸ª")
    print(f"   - å…³é”®å­—æ®µ: {', '.join(FIELDS_TO_ENUMERATE_COMPLETELY[:6])}...")
    
    all_results = {}
    
    for db_name, mdb_path in MDB_FILES.items():
        if not Path(mdb_path).exists():
            print(f"\n[é”™è¯¯] æ–‡ä»¶ä¸å­˜åœ¨: {mdb_path}")
            continue
        
        database_info = extract_all_constraints(mdb_path, db_name)
        if database_info:
            all_results[db_name] = database_info
            save_results(database_info)
    
    print("\n" + "="*80)
    print("[å®Œæˆ] æ‰€æœ‰åˆ†æå®Œæˆï¼")
    print("="*80)
    print("\n[è¾“å‡º] æ–‡ä»¶ä½äº: analysis_results/ ç›®å½•")
    print("\n[æ–‡ä»¶ç±»å‹]")
    print("   1. JSONæ ¼å¼ - å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®")
    print("   2. Markdownæ ¼å¼ - äººç±»å¯è¯»çš„åˆ†ææŠ¥å‘Š")
    print("   3. CSVæ ¼å¼ - å…³é”®å­—æ®µçš„å®Œæ•´æšä¸¾å€¼")
    print("\n[é‡ç‚¹] ç‰¹åˆ«å…³æ³¨ SUBGROUP_CODE å’Œ SUBGROUP_NAME çš„å®Œæ•´æšä¸¾å€¼ï¼")


if __name__ == '__main__':
    main()

